# Student Performance ETL Pipeline  

## ğŸ“Œ Overview  
This project implements an **ETL (Extract, Transform, Load) pipeline** in Python for building a **Student Performance Data Warehouse**.  
The pipeline integrates student, course, and grade data from multiple sources, transforms it into a clean star schema, and loads it into a **SQL Server database**.  

The goal is to support reporting and analytics on student grades, attendance, and course performance.  

---

## ğŸ—‚ï¸ Star Schema Design  
The data warehouse follows a **star schema** design:  

- **Dimension Tables**  
  - `dim_student`: Stores student information (ID, name, gender, city).  
  - `dim_course`: Stores course details (ID, name, credits).  
  - `dim_date`: Stores calendar attributes (date key, year, month, day).  

- **Fact Table**  
  - `fact_student_performance`: Stores student grades and attendance linked to student, course, and date dimensions.  

ğŸ“‚ Schema script: [`create_tables.sql`](create_tables.sql)  

### ERD Diagram  
![Star Schema](erd.png)  

---

## ğŸ”„ ETL Workflow  

### 1. Extract  
- **Students** â†’ from a text file (`students.txt`) using `pandas.read_csv` with a `|` delimiter.  
- **Grades** â†’ from a JSON API export (`api_grades.json`).  
- **Courses** â†’ from a SQLite database (`courses.db`).  

### 2. Transform  
- Cleans and validates student, course, and grade records.  
- Joins students, courses, and grades to create the **fact table**.  
- Generates the **date dimension** from grade dates (`DateKey`, Year, Month, Day).  
- Ensures all records have valid foreign keys to dimensions.  

### 3. Load  
- Connects to SQL Server using **pyodbc**.  
- Loads dimension data into:  
  - `dim_student`  
  - `dim_course`  
  - `dim_date`  
- Inserts fact data into:  
  - `fact_student_performance`  

ğŸ““ Pipeline implementation: [`ETL.ipynb`](ETL.ipynb)  

---

## âš™ï¸ Setup Instructions  

### 1. Prerequisites  
- Python 3.8+  
- SQL Server (or compatible RDBMS)  
- Required Python libraries:  
  ```bash
  pip install pandas pyodbc sqlalchemy
  
### 2. Create Database & Tables
  Run the SQL script in SQL Server Management Studio (SSMS): 
   ```bash
  CREATE DATABASE StudentPerformanceDW;
  USE StudentPerformanceDW;

### 3. Run the ETL Pipeline
  Open and execute the Jupyter Notebook:
  ```bash
  jupyter notebook ETL.ipynb
---
## ğŸ“‚ Project Structure  
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ students.txt # Student data (CSV format with "|" delimiter)
â”‚ â”œâ”€â”€ api_grades.json # Grades data (JSON)
â”‚ â”œâ”€â”€ courses.db # Courses database (SQLite)
â”œâ”€â”€ ETL.ipynb # ETL pipeline (Python Jupyter Notebook)
â”œâ”€â”€ create_tables.sql # SQL script for schema creation
â”œâ”€â”€ erd.png # ERD diagram (star schema)
â””â”€â”€ README.md # Project documentation

---

## ğŸš€ Future Improvements  
- Automate ETL execution with **Airflow** or **Prefect**.  
- Add **data validation and logging**.  
- Support for **incremental loads**.  
- Visualization dashboards (e.g., **Power BI / Tableau**).  

